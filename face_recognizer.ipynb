{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torchvision.transforms.autoaugment import AutoAugmentPolicy, AutoAugment\n",
    "\n",
    "from torchsampler.imbalanced import ImbalancedDatasetSampler\n",
    "\n",
    "train_dir = './train_images'\n",
    "test_dir = './test_images'\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.AutoAugment(AutoAugmentPolicy.CIFAR10),\n",
    "    transforms.Grayscale(),\n",
    "    transforms.ToTensor(), \n",
    "    transforms.Normalize(mean=(0,),std=(1,))])\n",
    "\n",
    "train_data = torchvision.datasets.ImageFolder(train_dir, transform=transform)\n",
    "test_data = torchvision.datasets.ImageFolder(test_dir, transform=transform)\n",
    "\n",
    "# 20% of the train dataset will be used as a validation exercise\n",
    "valid_size = 0.2\n",
    "batch_size = 32\n",
    "\n",
    "num_train = len(train_data)\n",
    "# (0, 1, 2, 3, ..., num_train)\n",
    "indices_train = list(range(num_train))\n",
    "# Reorders the indexes randomly (so we get 7, 4, 19...)\n",
    "np.random.shuffle(indices_train)\n",
    "\n",
    "# The first valid_size% of indexes will be for validation purposes\n",
    "split_tv = int(np.floor(valid_size * num_train))\n",
    "\n",
    "# Get the indexes, split between training and validation\n",
    "train_new_idx, valid_idx = indices_train[split_tv:],indices_train[:split_tv]\n",
    "\n",
    "train_sampler = ImbalancedDatasetSampler(train_data,train_new_idx)\n",
    "valid_sampler = ImbalancedDatasetSampler(train_data,valid_idx)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, sampler=train_sampler, num_workers=1)\n",
    "valid_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, sampler=valid_sampler, num_workers=1)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=True, num_workers=1)\n",
    "classes = ('noface','face')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 18, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(18, 32, 5)\n",
    "        self.fc1 = nn.Linear(32* 6 * 6, 48)\n",
    "        self.fc2 = nn.Linear(48, 32)\n",
    "        self.fc3 = nn.Linear(32, 16)\n",
    "        self.fc4 = nn.Linear(16, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 32 * 6 * 6)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "network = Net()\n",
    "network.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(network.parameters(), lr = 0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "0 398 - Training loss:  0.6931013821065426  - Validation loss: 0.6929158996206543\n",
      "0 798 - Training loss:  0.692806079685688  - Validation loss: 0.6925910791245903\n",
      "0 1198 - Training loss:  0.6924880911409855  - Validation loss: 0.6923556515771753\n",
      "0 1598 - Training loss:  0.6920529252290726  - Validation loss: 0.6916501319574562\n",
      "0 1998 - Training loss:  0.691294252127409  - Validation loss: 0.6906279537320552\n",
      "1 398 - Training loss:  0.6866407275199891  - Validation loss: 0.6827590084449755\n",
      "1 798 - Training loss:  0.6730063183605671  - Validation loss: 0.6489783072720837\n",
      "1 1198 - Training loss:  0.5660219924896955  - Validation loss: 0.45718664091846256\n",
      "1 1598 - Training loss:  0.4131418836861849  - Validation loss: 0.36497460445875907\n",
      "1 1998 - Training loss:  0.36606913324445484  - Validation loss: 0.3846292188618241\n",
      "2 398 - Training loss:  0.3310965881496668  - Validation loss: 0.30338919496338956\n",
      "2 798 - Training loss:  0.3027982799336314  - Validation loss: 0.28901700922109524\n",
      "2 1198 - Training loss:  0.2737776727415621  - Validation loss: 0.2525669740519457\n",
      "2 1598 - Training loss:  0.24728441135957838  - Validation loss: 0.2229825319001899\n",
      "2 1998 - Training loss:  0.22193770276382566  - Validation loss: 0.2049526454692622\n",
      "3 398 - Training loss:  0.1919915762078017  - Validation loss: 0.16524204461858039\n",
      "3 798 - Training loss:  0.16728271139785647  - Validation loss: 0.1597685504360741\n",
      "3 1198 - Training loss:  0.15999404971953482  - Validation loss: 0.15705355287623635\n",
      "3 1598 - Training loss:  0.1570322079001926  - Validation loss: 0.157527802307613\n",
      "3 1998 - Training loss:  0.1432307820348069  - Validation loss: 0.1369457189147831\n",
      "4 398 - Training loss:  0.12734761186875404  - Validation loss: 0.1159699494051601\n",
      "4 798 - Training loss:  0.12766559660201893  - Validation loss: 0.11711732866258753\n",
      "4 1198 - Training loss:  0.1262643239926547  - Validation loss: 0.12279406338702978\n",
      "4 1598 - Training loss:  0.11281546775018797  - Validation loss: 0.11181534179048255\n",
      "4 1998 - Training loss:  0.10908204918494448  - Validation loss: 0.10934558066860724\n",
      "5 398 - Training loss:  0.10034190969425254  - Validation loss: 0.13391871249403817\n",
      "5 798 - Training loss:  0.10220586607814766  - Validation loss: 0.09237408070932127\n",
      "5 1198 - Training loss:  0.0945324204931967  - Validation loss: 0.08763763945361515\n",
      "5 1598 - Training loss:  0.09885230930522085  - Validation loss: 0.09233528352991552\n",
      "5 1998 - Training loss:  0.09229672243294772  - Validation loss: 0.09063401250679773\n",
      "6 398 - Training loss:  0.09167138303630054  - Validation loss: 0.08382797926560512\n",
      "6 798 - Training loss:  0.08479923414066434  - Validation loss: 0.09383854676137\n",
      "6 1198 - Training loss:  0.09243249758379533  - Validation loss: 0.08002722335213516\n",
      "6 1598 - Training loss:  0.08461680557811632  - Validation loss: 0.09854373084835527\n",
      "6 1998 - Training loss:  0.08287240252131596  - Validation loss: 0.08875629701489382\n",
      "7 398 - Training loss:  0.07905598477984313  - Validation loss: 0.07202283656897951\n",
      "7 798 - Training loss:  0.07100024229614064  - Validation loss: 0.08195179959398918\n",
      "7 1198 - Training loss:  0.07480501005367841  - Validation loss: 0.07096693099059412\n",
      "7 1598 - Training loss:  0.07503495827142614  - Validation loss: 0.0640281714024006\n",
      "7 1998 - Training loss:  0.07359884358069393  - Validation loss: 0.0754500971421541\n",
      "8 398 - Training loss:  0.06404138299170882  - Validation loss: 0.06745724707348223\n",
      "8 798 - Training loss:  0.06779263543721754  - Validation loss: 0.06995800978062798\n",
      "8 1198 - Training loss:  0.06794104467786383  - Validation loss: 0.0819362931084115\n",
      "8 1598 - Training loss:  0.06243712117517134  - Validation loss: 0.06043649585925705\n",
      "8 1998 - Training loss:  0.06171427594294073  - Validation loss: 0.06845055269815664\n",
      "9 398 - Training loss:  0.06462827217910672  - Validation loss: 0.05992458859782663\n",
      "9 798 - Training loss:  0.06658225122722797  - Validation loss: 0.07101030458184283\n",
      "9 1198 - Training loss:  0.055793967038043776  - Validation loss: 0.06067112932924363\n",
      "9 1598 - Training loss:  0.05646357669713325  - Validation loss: 0.06282800734116134\n",
      "9 1998 - Training loss:  0.0596598771729623  - Validation loss: 0.05341121091674494\n",
      "10 398 - Training loss:  0.055546114895551  - Validation loss: 0.06296914079809035\n",
      "10 798 - Training loss:  0.06008470343891531  - Validation loss: 0.054749577353669504\n",
      "10 1198 - Training loss:  0.056602115663699805  - Validation loss: 0.055430239588525214\n",
      "10 1598 - Training loss:  0.06123113680136157  - Validation loss: 0.06101875361053108\n",
      "10 1998 - Training loss:  0.05540650582552189  - Validation loss: 0.051801222805984364\n",
      "11 398 - Training loss:  0.05764775947725866  - Validation loss: 0.053011999134938086\n",
      "11 798 - Training loss:  0.05175425063614966  - Validation loss: 0.057844449727699074\n",
      "11 1198 - Training loss:  0.052455969477596225  - Validation loss: 0.053000531373208856\n",
      "11 1598 - Training loss:  0.05279137542616809  - Validation loss: 0.0500475991905086\n",
      "11 1998 - Training loss:  0.053411762474424904  - Validation loss: 0.058351522050343635\n"
     ]
    }
   ],
   "source": [
    "epoch = 1\n",
    "max_epoch = 12\n",
    "print_every_n_batch = 400\n",
    "\n",
    "best_network: Net = network\n",
    "best_loss_validation = 999999999\n",
    "steps_since_last_best = 0\n",
    "threshold_early_stopping = 4 # Stop after 4 iterations without a new best network\n",
    "best_model_found = False\n",
    "\n",
    "print(device)\n",
    "\n",
    "while not best_model_found and epoch <= max_epoch:\n",
    "\n",
    "    total_loss_training = 0.0\n",
    "    i = 0\n",
    "    \n",
    "    for data_training, target_training in train_loader:\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        inputs, labels = data_training.to(device), target_training.to(device)\n",
    "\n",
    "        outputs = network(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss_training += loss.item()\n",
    "\n",
    "        #Every 400 batches, we test the model on the validation data\n",
    "        if i % print_every_n_batch == print_every_n_batch - 1:    # print every n mini-batches\n",
    "            running_loss_training = total_loss_training / print_every_n_batch\n",
    "            total_loss_training = 0.0\n",
    "            total_loss_validation = 0.0\n",
    "\n",
    "\n",
    "            # We feed the validation data to the network\n",
    "            for data_valid, labels_valid in valid_loader:\n",
    "                images, labels = data_valid.to(device), labels_valid.to(device)\n",
    "                outputs = network(images)\n",
    "                loss = criterion(outputs,labels)\n",
    "                total_loss_validation += loss.item()\n",
    "            \n",
    "            running_loss_validation = total_loss_validation / len(valid_loader)\n",
    "\n",
    "            print(epoch, i+1, \"-\", \"Training loss: \", running_loss_training, \" - Validation loss:\", running_loss_validation)\n",
    "            \n",
    "            if(running_loss_validation < best_loss_validation):\n",
    "                best_loss_validation = running_loss_validation\n",
    "                best_network = network\n",
    "                steps_since_last_best = 0\n",
    "            else:\n",
    "                steps_since_last_best += 1\n",
    "\n",
    "            if steps_since_last_best >= threshold_early_stopping:\n",
    "                best_model_found = True\n",
    "                break\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    epoch += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0500475991905086\n"
     ]
    }
   ],
   "source": [
    "print(best_loss_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 95.90 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for imgs, labls in test_loader:\n",
    "        images, labels = imgs.to(device), labls.to(device)\n",
    "        outputs = best_network(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the 10000 test images: %2.2f %%' % (\n",
    "    100 * correct / total))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c56874b59277349bf472b3f19dc5e9ff1b8fe11858b83d154515d52757e9b043"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
