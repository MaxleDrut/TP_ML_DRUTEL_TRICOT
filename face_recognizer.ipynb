{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torchvision.transforms.autoaugment import AutoAugmentPolicy, AutoAugment\n",
    "\n",
    "from torchsampler.imbalanced import ImbalancedDatasetSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_dir = './train_images'\n",
    "test_dir = './test_images'\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.AutoAugment(AutoAugmentPolicy.CIFAR10),\n",
    "    transforms.Grayscale(),\n",
    "    transforms.ToTensor(), \n",
    "    transforms.Normalize(mean=(0,),std=(1,))])\n",
    "\n",
    "train_data = torchvision.datasets.ImageFolder(train_dir, transform=transform)\n",
    "test_data = torchvision.datasets.ImageFolder(test_dir, transform=transform)\n",
    "\n",
    "# 20% of the train dataset will be used as a validation exercise\n",
    "valid_size = 0.2\n",
    "batch_size = 32\n",
    "\n",
    "num_train = len(train_data)\n",
    "# (0, 1, 2, 3, ..., num_train)\n",
    "indices_train = list(range(num_train))\n",
    "# Reorders the indexes randomly (so we get 7, 4, 19...)\n",
    "np.random.shuffle(indices_train)\n",
    "\n",
    "# The first valid_size% of indexes will be for validation purposes\n",
    "split_tv = int(np.floor(valid_size * num_train))\n",
    "\n",
    "# Get the indexes, split between training and validation\n",
    "train_new_idx, valid_idx = indices_train[split_tv:],indices_train[:split_tv]\n",
    "\n",
    "train_sampler = ImbalancedDatasetSampler(train_data,train_new_idx)\n",
    "valid_sampler = ImbalancedDatasetSampler(train_data,valid_idx)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, sampler=train_sampler, num_workers=1)\n",
    "valid_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, sampler=valid_sampler, num_workers=1)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=True, num_workers=1)\n",
    "classes = ('noface','face')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 18, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(18, 32, 5)\n",
    "        self.fc1 = nn.Linear(32* 6 * 6, 48)\n",
    "        self.fc2 = nn.Linear(48, 32)\n",
    "        self.fc3 = nn.Linear(32, 16)\n",
    "        self.fc4 = nn.Linear(16, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 32 * 6 * 6)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "network = Net()\n",
    "network.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(network.parameters(), lr = 0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 400 - Training loss:  0.08074017538325279  - Validation loss: 0.08339823771471755\n",
      "1 800 - Training loss:  0.08476066324597924  - Validation loss: 0.07393808361321959\n",
      "1 1200 - Training loss:  0.07975487063100445  - Validation loss: 0.08112689075358326\n",
      "1 1600 - Training loss:  0.07871337978111115  - Validation loss: 0.07604924955657595\n",
      "1 2000 - Training loss:  0.07170392604530207  - Validation loss: 0.07976077160780165\n",
      "2 400 - Training loss:  0.07841838797350648  - Validation loss: 0.07872794040146962\n"
     ]
    }
   ],
   "source": [
    "epoch = 1\n",
    "max_epoch = 4\n",
    "print_every_n_batch = 400\n",
    "\n",
    "best_network: Net = network\n",
    "best_loss_validation = 999999999\n",
    "steps_since_last_best = 0\n",
    "threshold_early_stopping = 4 # Stop after 4 iterations without a new best network\n",
    "best_model_found = False\n",
    "\n",
    "while not best_model_found and epoch <= max_epoch:\n",
    "\n",
    "    total_loss_training = 0.0\n",
    "    i = 0\n",
    "    \n",
    "    for data_training, target_training in train_loader:\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        inputs, labels = data_training.to(device), target_training.to(device)\n",
    "\n",
    "        outputs = network(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss_training += loss.item()\n",
    "\n",
    "        #Every 400 batches, we test the model on the validation data\n",
    "        if i % print_every_n_batch == print_every_n_batch - 1:    # print every n mini-batches\n",
    "            running_loss_training = total_loss_training / print_every_n_batch\n",
    "            total_loss_training = 0.0\n",
    "            total_loss_validation = 0.0\n",
    "\n",
    "\n",
    "            # We feed the validation data to the network\n",
    "            for data_valid, labels_valid in valid_loader:\n",
    "                images, labels = data_valid.to(device), labels_valid.to(device)\n",
    "                outputs = network(images)\n",
    "                loss = criterion(outputs,labels)\n",
    "                total_loss_validation += loss.item()\n",
    "            \n",
    "            running_loss_validation = total_loss_validation / len(valid_loader)\n",
    "\n",
    "            print(epoch, i+1, \"-\", \"Training loss: \", running_loss_training, \" - Validation loss:\", running_loss_validation)\n",
    "            \n",
    "            if(running_loss_validation < best_loss_validation):\n",
    "                best_loss_validation = running_loss_validation\n",
    "                best_network = network\n",
    "                steps_since_last_best = 0\n",
    "            else:\n",
    "                steps_since_last_best += 1\n",
    "\n",
    "            if steps_since_last_best >= threshold_early_stopping:\n",
    "                best_model_found = True\n",
    "                break\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    epoch += 1\n",
    "\n",
    "network = best_network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.06220278132089109\n"
     ]
    }
   ],
   "source": [
    "print(best_loss_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the model\n",
    "torch.save(network.state_dict(), \"./face_recognizer.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (conv1): Conv2d(1, 18, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2): Conv2d(18, 32, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (fc1): Linear(in_features=1152, out_features=48, bias=True)\n",
       "  (fc2): Linear(in_features=48, out_features=32, bias=True)\n",
       "  (fc3): Linear(in_features=32, out_features=16, bias=True)\n",
       "  (fc4): Linear(in_features=16, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading the model\n",
    "network = Net()\n",
    "network.load_state_dict(torch.load(\"./face_recognizer.pt\"))\n",
    "\n",
    "network.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 95.88 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for imgs, labls in test_loader:\n",
    "        images, labels = imgs.to(device), labls.to(device)\n",
    "        outputs = network(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the 10000 test images: %2.2f %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2 as cv\n",
    "from PIL import Image\n",
    "\n",
    "original_image = cv.imread('./corpo.png')\n",
    "\n",
    "# Create the sub-images of the original image with the sliding window\n",
    "window_size_x = 36\n",
    "window_size_y = 36\n",
    "face_radius = window_size_x\n",
    "total_scaling = 1\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [ transforms.Grayscale(),\n",
    "    transforms.ToTensor(), \n",
    "    transforms.Normalize(mean=(0,),std=(1,))])\n",
    "\n",
    "\n",
    "to_detect = original_image \n",
    "\n",
    "network.to(device)\n",
    "\n",
    "while(to_detect.shape[0] > window_size_y and to_detect.shape[1] > window_size_x) :\n",
    "    for y_window in range(0, to_detect.shape[0],window_size_y):\n",
    "        for x_window in range(0, to_detect.shape[1], window_size_x):\n",
    "            # Extract image:\n",
    "            sub_image = to_detect[\n",
    "                y_window:y_window+window_size_y,\n",
    "                x_window:x_window+window_size_x,\n",
    "            ]\n",
    "            sub_image_PIL = Image.fromarray(sub_image)\n",
    "            sub_image_PIL = sub_image_PIL.resize((36,36))\n",
    "            sub_image_PIL = transform(sub_image_PIL)\n",
    "\n",
    "            input = sub_image_PIL.to(device)\n",
    "            outputs = network(input)\n",
    "            predicted = torch.softmax(outputs.data, 1)\n",
    "            face_probability = predicted[0][1].item()\n",
    "\n",
    "            if(face_probability > 0.20):\n",
    "                x_back_to_scale = int(x_window/total_scaling)\n",
    "                y_back_to_scale = int(y_window+18/total_scaling)\n",
    "                radius_scaled = int(face_radius/total_scaling)\n",
    "                cv.rectangle(\n",
    "                    original_image, \n",
    "                    (x_back_to_scale, y_back_to_scale),\n",
    "                    (x_back_to_scale+radius_scaled, y_back_to_scale+radius_scaled),\n",
    "                    (0, 255, 0),\n",
    "                    2\n",
    "                )\n",
    "\n",
    "            x_window += 1\n",
    "        \n",
    "        y_window += 1\n",
    "     \n",
    "    step_scaling = 0.95   # percent of original size\n",
    "    total_scaling = total_scaling*step_scaling\n",
    "    width = int(to_detect.shape[1] * step_scaling)\n",
    "    height = int(to_detect.shape[0] * step_scaling)\n",
    "    dim = (width, height)\n",
    "\n",
    "    to_detect = cv.resize(to_detect, dim, interpolation = cv.INTER_AREA)\n",
    "\n",
    "cv.imshow(\"img\",original_image)\n",
    "cv.waitKey(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that our recognizer finds the faces, but also a lot of non faces.\n",
    "We therefore give the image of our hikers in the forest without their faces, and use the mini patches as pre-labelled dataset to retrain the model (as we know they will not have a face)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the new sub-images\n",
    "import os\n",
    "\n",
    "# Load the images of the folder\n",
    "texture_directory = 'texture_images'\n",
    "texture_images = [] \n",
    "# iterate over files in\n",
    "# that directory\n",
    "for filename in os.listdir(texture_directory):\n",
    "    f = os.path.join(texture_directory, filename)\n",
    "    # checking if it is a file\n",
    "    if os.path.isfile(f):\n",
    "        texture_images.append(cv.imread(f))\n",
    "\n",
    "# Create the sub-images of the original image with the sliding window\n",
    "window_size_x = 36\n",
    "window_size_y = 36\n",
    "face_radius = window_size_x\n",
    "total_scaling = 1\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [ transforms.Grayscale(),\n",
    "    transforms.ToTensor(), \n",
    "    transforms.Normalize(mean=(0,),std=(1,))])\n",
    "\n",
    "\n",
    "step_scaling = 0.90\n",
    "face_probability_thresh = 0.1\n",
    "image_number = 1\n",
    "\n",
    "for to_detect in texture_images:\n",
    "    while(to_detect.shape[0] > window_size_y and to_detect.shape[1] > window_size_x) :\n",
    "        for y_window in range(0, to_detect.shape[0],window_size_y):\n",
    "            for x_window in range(0, to_detect.shape[1], window_size_x):\n",
    "                # Extract image:\n",
    "                sub_image = to_detect[\n",
    "                    y_window:y_window+window_size_y,\n",
    "                    x_window:x_window+window_size_x,\n",
    "                ]\n",
    "\n",
    "                sub_image = cv.resize(sub_image, (36, 36))\n",
    "                sub_image_PIL = Image.fromarray(sub_image)\n",
    "                sub_image_PIL = sub_image_PIL.resize((36,36))\n",
    "                sub_image_PIL = transform(sub_image_PIL)\n",
    "\n",
    "                input = sub_image_PIL.to(device)\n",
    "                outputs = network(input)\n",
    "                predicted = torch.softmax(outputs.data, 1)\n",
    "                face_probability = predicted[0][1].item()\n",
    "\n",
    "                if(face_probability > face_probability_thresh):\n",
    "                    sub_image_name = \"./retrain_dataset/0/\" + str(image_number) + \"-\" + str(x_window) + \"-\" + str(y_window) + \"x\" + str(total_scaling) + \".pgm\"\n",
    "                    sub_image = cv.cvtColor(sub_image, cv.COLOR_BGR2GRAY)\n",
    "                    cv.imwrite(sub_image_name, sub_image)\n",
    "        \n",
    "                x_window += 1\n",
    "            y_window += 1\n",
    "        \n",
    "        total_scaling = total_scaling*step_scaling\n",
    "        width = int(to_detect.shape[1] * step_scaling)\n",
    "        height = int(to_detect.shape[0] * step_scaling)\n",
    "        dim = (width, height)\n",
    "        to_detect = cv.resize(to_detect, dim, interpolation = cv.INTER_AREA)\n",
    "    \n",
    "    image_number += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list.append() takes exactly one argument (0 given)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [51], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[39m# checking if it is a file\u001b[39;00m\n\u001b[0;32m     12\u001b[0m     \u001b[39mif\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39misfile(f):\n\u001b[1;32m---> 13\u001b[0m         texture_images\u001b[39m.\u001b[39;49mappend()\n\u001b[0;32m     15\u001b[0m \u001b[39m# Create the sub-images of the original image with the sliding window\u001b[39;00m\n\u001b[0;32m     16\u001b[0m window_size_x \u001b[39m=\u001b[39m \u001b[39m36\u001b[39m\n",
      "\u001b[1;31mTypeError\u001b[0m: list.append() takes exactly one argument (0 given)"
     ]
    }
   ],
   "source": [
    "# Create the new sub-images\n",
    "import os\n",
    "\n",
    "# Load the images of the folder\n",
    "texture_directory = 'texture_images'\n",
    "texture_images = [] \n",
    "# iterate over files in\n",
    "# that directory\n",
    "for filename in os.listdir(texture_directory):\n",
    "    f = os.path.join(texture_directory, filename)\n",
    "    # checking if it is a file\n",
    "    if os.path.isfile(f):\n",
    "        texture_images.append(f)\n",
    "\n",
    "# Create the sub-images of the original image with the sliding window\n",
    "window_size_x = 36\n",
    "window_size_y = 36\n",
    "face_radius = window_size_x\n",
    "total_scaling = 1\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [ transforms.Grayscale(),\n",
    "    transforms.ToTensor(), \n",
    "    transforms.Normalize(mean=(0,),std=(1,))])\n",
    "\n",
    "\n",
    "to_detect = original_image \n",
    "\n",
    "network.to(device)\n",
    "\n",
    "while(to_detect.shape[0] > window_size_y and to_detect.shape[1] > window_size_x) :\n",
    "    for y_window in range(0, to_detect.shape[0],window_size_y):\n",
    "        for x_window in range(0, to_detect.shape[1], window_size_x):\n",
    "            # Extract image:\n",
    "            sub_image = to_detect[\n",
    "                y_window:y_window+window_size_y,\n",
    "                x_window:x_window+window_size_x,\n",
    "            ]\n",
    "            sub_image = cv.resize(sub_image, (36, 36))\n",
    "            sub_image = cv.cvtColor(sub_image, cv.COLOR_BGR2GRAY)\n",
    "\n",
    "            image_name = \"./train_images/0/\" + str(x_window) + \"-\" + str(y_window) + \"x\" + str(total_scaling) + \".pgm\"\n",
    "            cv.imwrite(image_name, sub_image)\n",
    "    \n",
    "            x_window += 4\n",
    "        y_window += 4\n",
    "     \n",
    "    step_scaling = 0.98   # percent of original size\n",
    "    total_scaling = total_scaling*step_scaling\n",
    "    width = int(to_detect.shape[1] * step_scaling)\n",
    "    height = int(to_detect.shape[0] * step_scaling)\n",
    "    dim = (width, height)\n",
    "    to_detect = cv.resize(to_detect, dim, interpolation = cv.INTER_AREA)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c56874b59277349bf472b3f19dc5e9ff1b8fe11858b83d154515d52757e9b043"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
